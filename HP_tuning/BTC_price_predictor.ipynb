{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gBwiKQGnerN-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from copy import deepcopy\n",
        "import itertools\n",
        "from PriceData import DataProcessing\n",
        "from LSTM import LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MARSvkGrerOD"
      },
      "source": [
        "### Model parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FgLSxzsverOE"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "NUM_EPOCHS      = 1000\n",
        "LEARNING_RATE   = 0.0001\n",
        "\n",
        "HIDDEN_SIZE     = 32            # Dimensionality of hidden units\n",
        "NUM_FC_FEATURES = 128           # Number of output features, first FF layer\n",
        "NUM_LSTM_LAYERS = 2             # Number of LSTM Layers\n",
        "\n",
        "SEQ_LENGTH      = 60            # Length of inputs sequences\n",
        "BATCH_SIZE      = 16\n",
        "\n",
        "# Regular parameters\n",
        "input_size  = 1      # Number of input features\n",
        "output_size = 1      # Number of output features\n",
        "\n",
        "criterion = nn.MSELoss()    # Set MSE as loss function\n",
        "\n",
        "nr_folds = 10               # Number of folds in K-fold cross validation\n",
        "kf = KFold(n_splits = nr_folds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wGmuK9vgerOF"
      },
      "outputs": [],
      "source": [
        "learning_rates  = [0.01, 0.001, 0.0001, 0.00001]\n",
        "batch_sizes     = [32, 64, 128, 256, 512]\n",
        "hidden_sizes    = [16, 32, 48, 60]\n",
        "fc_features     = [64, 96, 128, 160]\n",
        "num_lstms       = [2,4,6,8,10]\n",
        "seq_lengths     = [30, 60, 90, 120]\n",
        "\n",
        "HP_combinations = list(itertools.product(seq_lengths, batch_sizes, learning_rates,\n",
        "                                         fc_features, hidden_sizes, num_lstms))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic-CixCyerOF"
      },
      "source": [
        "### Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "data_process = DataProcessing(seq_length=SEQ_LENGTH, batch_size=BATCH_SIZE)\n",
        "X_train, y_train, X_test, y_test = data_process.get_process_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoookzoYerOP"
      },
      "source": [
        "### LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1GgiWuK3erOP"
      },
      "outputs": [],
      "source": [
        "# Set device to CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLEZuaT0erOR"
      },
      "source": [
        "### Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ksVtGXz8erOR"
      },
      "outputs": [],
      "source": [
        "## Struct to store model info\n",
        "class ModelInfo():\n",
        "    def __init__(self):\n",
        "        self.inputs     = []\n",
        "        self.labels     = []\n",
        "        self.outputs    = []\n",
        "        self.loss       = float(0)\n",
        "        self.params     = None\n",
        "        self.lstm       = None\n",
        "        self.train_loader  = None\n",
        "        self.val_loader    = None\n",
        "        self.input_scaler  = None\n",
        "        self.output_scaler = None\n",
        "\n",
        "# Store best model of every fold\n",
        "results = {i:ModelInfo() for i in range(nr_folds)}\n",
        "for i in range(nr_folds): results[i].loss = np.inf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1RLygOC6erOR"
      },
      "outputs": [],
      "source": [
        "class Hyperparams():\n",
        "    def __init__(self, learning_rate, hidden_size, num_fc, num_lstm):\n",
        "        self.learning_rate  = learning_rate\n",
        "        self.hidden_size    = hidden_size\n",
        "        self.num_fc         = num_fc\n",
        "        self.num_lstm       = num_lstm\n",
        "\n",
        "        self.avg_val_error  = float(0.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4vYirhP8erOS",
        "outputId": "893208e8-b520-4237-ed33-150e69597344"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "FOLD 0\n",
            "epoch 0 | validation loss: 0.001761812313426552 | new best model!\n",
            "epoch 1 | validation loss: 0.018288477889395187\n",
            "epoch 2 | validation loss: 0.06086118120167937\n",
            "epoch 3 | validation loss: 0.07532273924776486\n",
            "epoch 4 | validation loss: 0.0525768439152411\n",
            "epoch 5 | validation loss: 0.019067051487841775\n",
            "epoch 6 | validation loss: 0.01105225134441363\n",
            "epoch 7 | validation loss: 0.007463814491139991\n",
            "epoch 8 | validation loss: 0.0048502348363399506\n",
            "epoch 9 | validation loss: 0.003779321805008554\n",
            "epoch 10 | validation loss: 0.0030623741227567996\n",
            "epoch 11 | validation loss: 0.002455197337050257\n",
            "epoch 12 | validation loss: 0.0019417295822807188\n",
            "epoch 13 | validation loss: 0.0015008760663996717 | new best model!\n",
            "epoch 14 | validation loss: 0.0011887966533582325 | new best model!\n",
            "epoch 15 | validation loss: 0.0008943323916485367 | new best model!\n",
            "epoch 16 | validation loss: 0.0011996896085163047\n",
            "epoch 17 | validation loss: 0.000772604310701094 | new best model!\n",
            "epoch 18 | validation loss: 0.0012374347352306359\n",
            "epoch 19 | validation loss: 0.0009465914258076477\n",
            "epoch 20 | validation loss: 0.0011231248603767849\n",
            "epoch 21 | validation loss: 0.0008453046076673283\n",
            "epoch 22 | validation loss: 0.0005212592857008401 | new best model!\n",
            "epoch 23 | validation loss: 0.0005106517785731869 | new best model!\n",
            "epoch 24 | validation loss: 0.00046515670497423604 | new best model!\n",
            "epoch 25 | validation loss: 0.0005885922985596283\n",
            "epoch 26 | validation loss: 0.0008194319198082667\n",
            "epoch 27 | validation loss: 0.0006574847765088114\n",
            "epoch 28 | validation loss: 0.0006201005812077451\n",
            "epoch 29 | validation loss: 0.0004897339253407804\n",
            "epoch 30 | validation loss: 0.0005550319539387212\n",
            "epoch 31 | validation loss: 0.0003038089995633137 | new best model!\n",
            "epoch 32 | validation loss: 0.0003168801205382416\n",
            "epoch 33 | validation loss: 0.0003823283035931776\n",
            "epoch 34 | validation loss: 0.00029949787494842894 | new best model!\n",
            "epoch 35 | validation loss: 0.0003559368151400122\n",
            "epoch 36 | validation loss: 0.0002710461594038601 | new best model!\n",
            "epoch 37 | validation loss: 0.00020039258119400723 | new best model!\n",
            "epoch 38 | validation loss: 0.00026282322427765133\n",
            "epoch 39 | validation loss: 0.000303328188497939\n",
            "epoch 40 | validation loss: 0.0004264796251456054\n",
            "epoch 41 | validation loss: 0.00025731765994610863\n",
            "epoch 42 | validation loss: 0.0004144952209961567\n",
            "epoch 43 | validation loss: 0.00033021625040419167\n",
            "epoch 44 | validation loss: 0.0001812514673734508 | new best model!\n",
            "epoch 45 | validation loss: 0.0002198279414317637\n",
            "epoch 46 | validation loss: 0.00013571121027260751 | new best model!\n",
            "epoch 47 | validation loss: 0.0001783997149037272\n",
            "epoch 48 | validation loss: 0.00019604488690155058\n",
            "epoch 49 | validation loss: 0.0001964112507266691\n",
            "epoch 50 | validation loss: 0.00014679664081995725\n",
            "epoch 51 | validation loss: 0.00011482339492171636 | new best model!\n",
            "epoch 52 | validation loss: 0.00015903774757524452\n",
            "epoch 53 | validation loss: 0.00010114138331118738 | new best model!\n",
            "epoch 54 | validation loss: 0.0001412160772815696\n",
            "epoch 55 | validation loss: 0.00015526212512538353\n",
            "epoch 56 | validation loss: 0.00017201825202001992\n",
            "epoch 57 | validation loss: 0.0001840009176703461\n",
            "epoch 58 | validation loss: 0.0001557415746771897\n",
            "epoch 59 | validation loss: 0.00010845970207031184\n",
            "epoch 60 | validation loss: 9.458619459629907e-05 | new best model!\n",
            "epoch 61 | validation loss: 0.00016524981336780393\n",
            "epoch 62 | validation loss: 0.00010511988278137454\n",
            "epoch 63 | validation loss: 0.00011137481033074437\n",
            "epoch 64 | validation loss: 0.0001357818387727353\n",
            "epoch 65 | validation loss: 0.00012129732359816054\n",
            "epoch 66 | validation loss: 8.158732368558828e-05 | new best model!\n",
            "epoch 67 | validation loss: 0.00020653248507837167\n",
            "epoch 68 | validation loss: 9.088747252202925e-05\n",
            "epoch 69 | validation loss: 6.998806267282427e-05 | new best model!\n",
            "epoch 70 | validation loss: 0.00014763505792611146\n",
            "epoch 71 | validation loss: 8.193749044949072e-05\n",
            "epoch 72 | validation loss: 0.000173560211935962\n",
            "epoch 73 | validation loss: 0.0001031402223036691\n",
            "epoch 74 | validation loss: 0.0001958028711409757\n",
            "epoch 75 | validation loss: 0.00011248226798800585\n",
            "epoch 76 | validation loss: 8.110922018594075e-05\n",
            "epoch 77 | validation loss: 6.554639432059568e-05 | new best model!\n",
            "epoch 78 | validation loss: 0.0001860417225023931\n",
            "epoch 79 | validation loss: 7.940328331252593e-05\n",
            "epoch 80 | validation loss: 8.186144935150099e-05\n",
            "epoch 81 | validation loss: 6.525570565177727e-05 | new best model!\n",
            "epoch 82 | validation loss: 0.00015576218018915306\n",
            "epoch 83 | validation loss: 7.582989006290777e-05\n",
            "epoch 84 | validation loss: 0.00017039217553766712\n",
            "epoch 85 | validation loss: 9.544842915991987e-05\n",
            "epoch 86 | validation loss: 9.143370646467832e-05\n",
            "epoch 87 | validation loss: 5.818086778422834e-05 | new best model!\n",
            "epoch 88 | validation loss: 0.00015077815000300428\n",
            "epoch 89 | validation loss: 9.292462177005032e-05\n",
            "epoch 90 | validation loss: 6.994489857398938e-05\n",
            "epoch 91 | validation loss: 5.9100476586796245e-05\n",
            "epoch 92 | validation loss: 7.167157553210148e-05\n",
            "epoch 93 | validation loss: 4.92890584707441e-05 | new best model!\n",
            "epoch 94 | validation loss: 6.620584391774693e-05\n",
            "epoch 95 | validation loss: 6.598540293063187e-05\n",
            "epoch 96 | validation loss: 0.00012552788263876989\n",
            "epoch 97 | validation loss: 5.513266874653969e-05\n",
            "epoch 98 | validation loss: 7.013202223658612e-05\n",
            "epoch 99 | validation loss: 9.558857813577301e-05\n",
            "epoch 100 | validation loss: 0.00012039456851198338\n",
            "epoch 101 | validation loss: 4.25054788268296e-05 | new best model!\n",
            "epoch 102 | validation loss: 5.011358221703891e-05\n",
            "epoch 103 | validation loss: 6.583410199060122e-05\n",
            "epoch 104 | validation loss: 6.250383298070769e-05\n",
            "epoch 105 | validation loss: 4.446209056498317e-05\n",
            "epoch 106 | validation loss: 4.5594810412045005e-05\n",
            "epoch 107 | validation loss: 9.71215897932e-05\n",
            "epoch 108 | validation loss: 0.00011993738592406902\n",
            "epoch 109 | validation loss: 3.767530939415208e-05 | new best model!\n",
            "epoch 110 | validation loss: 8.636388936013515e-05\n",
            "epoch 111 | validation loss: 7.515744164265925e-05\n",
            "epoch 112 | validation loss: 8.734852794337453e-05\n",
            "epoch 113 | validation loss: 3.653062783217008e-05 | new best model!\n",
            "epoch 114 | validation loss: 5.808251275344186e-05\n",
            "epoch 115 | validation loss: 4.3739725437522535e-05\n",
            "epoch 116 | validation loss: 5.116834472573828e-05\n",
            "epoch 117 | validation loss: 0.0002506405778279129\n",
            "epoch 118 | validation loss: 0.00016813248138143017\n",
            "epoch 119 | validation loss: 8.891778790192413e-05\n",
            "epoch 120 | validation loss: 3.208833153881382e-05 | new best model!\n",
            "epoch 121 | validation loss: 0.00011921897774820016\n",
            "epoch 122 | validation loss: 2.982771288121252e-05 | new best model!\n",
            "epoch 123 | validation loss: 5.282382465208814e-05\n",
            "epoch 124 | validation loss: 6.589930879791999e-05\n",
            "epoch 125 | validation loss: 8.10907997999623e-05\n",
            "epoch 126 | validation loss: 2.778296951484143e-05 | new best model!\n",
            "epoch 127 | validation loss: 3.745492735366237e-05\n",
            "epoch 128 | validation loss: 4.7848091461284116e-05\n",
            "epoch 129 | validation loss: 5.361649209589814e-05\n",
            "epoch 130 | validation loss: 4.1296422507132646e-05\n",
            "epoch 131 | validation loss: 2.954239773446586e-05\n",
            "epoch 132 | validation loss: 3.4261827847460934e-05\n",
            "epoch 133 | validation loss: 3.429399521337473e-05\n",
            "epoch 134 | validation loss: 4.679359699366614e-05\n",
            "epoch 135 | validation loss: 2.5534804178408065e-05 | new best model!\n",
            "epoch 136 | validation loss: 9.202884185859668e-05\n",
            "epoch 137 | validation loss: 2.6551313599806497e-05\n",
            "epoch 138 | validation loss: 2.515857603871804e-05 | new best model!\n",
            "epoch 139 | validation loss: 0.00012906035044579767\n",
            "epoch 140 | validation loss: 3.8393625280670154e-05\n",
            "epoch 141 | validation loss: 4.233730702805717e-05\n",
            "epoch 142 | validation loss: 2.651191909665483e-05\n",
            "epoch 143 | validation loss: 2.7016640891425986e-05\n",
            "epoch 144 | validation loss: 5.261064572031111e-05\n",
            "epoch 145 | validation loss: 6.108873880553542e-05\n",
            "epoch 146 | validation loss: 3.0651244482474537e-05\n",
            "epoch 147 | validation loss: 2.7739650509569662e-05\n",
            "epoch 148 | validation loss: 4.013191314048267e-05\n",
            "epoch 149 | validation loss: 0.00012357181341420592\n",
            "epoch 150 | validation loss: 8.811432053335011e-05\n",
            "epoch 151 | validation loss: 6.264158426867133e-05\n",
            "epoch 152 | validation loss: 5.203139874408537e-05\n",
            "epoch 153 | validation loss: 3.333867838201903e-05\n",
            "epoch 154 | validation loss: 3.661755775803093e-05\n",
            "epoch 155 | validation loss: 2.429955338811851e-05 | new best model!\n",
            "epoch 156 | validation loss: 0.00012713417683179223\n",
            "epoch 157 | validation loss: 2.3737010256549445e-05 | new best model!\n",
            "epoch 158 | validation loss: 5.057805008488815e-05\n",
            "epoch 159 | validation loss: 2.893659939218196e-05\n",
            "epoch 160 | validation loss: 2.3684140582450448e-05 | new best model!\n",
            "epoch 161 | validation loss: 3.318694585427043e-05\n",
            "epoch 162 | validation loss: 6.0599255058540234e-05\n",
            "epoch 163 | validation loss: 2.5101237010208672e-05\n",
            "epoch 164 | validation loss: 2.6446389711054508e-05\n",
            "epoch 165 | validation loss: 2.4111224807451697e-05\n",
            "epoch 166 | validation loss: 3.581361534088501e-05\n",
            "epoch 167 | validation loss: 5.1939100883048794e-05\n",
            "epoch 168 | validation loss: 8.593302092257156e-05\n",
            "epoch 169 | validation loss: 3.083513833449355e-05\n",
            "epoch 170 | validation loss: 4.7773334050102024e-05\n",
            "epoch 171 | validation loss: 3.750275793988424e-05\n",
            "epoch 172 | validation loss: 2.1571157991015104e-05 | new best model!\n",
            "epoch 173 | validation loss: 4.5817668836077375e-05\n",
            "epoch 174 | validation loss: 2.8230209863977507e-05\n",
            "epoch 175 | validation loss: 4.4235676406450306e-05\n",
            "epoch 176 | validation loss: 3.4084569961123634e-05\n",
            "epoch 177 | validation loss: 2.814599724842992e-05\n",
            "epoch 178 | validation loss: 7.585693518714314e-05\n",
            "epoch 179 | validation loss: 6.408222932817255e-05\n",
            "epoch 180 | validation loss: 5.80889062413397e-05\n",
            "epoch 181 | validation loss: 2.694481517145115e-05\n",
            "epoch 182 | validation loss: 3.2894709160505696e-05\n",
            "epoch 183 | validation loss: 9.859997979739481e-05\n",
            "epoch 184 | validation loss: 0.0001351109767711023\n",
            "epoch 185 | validation loss: 3.0208496030224653e-05\n",
            "epoch 186 | validation loss: 2.970829502503745e-05\n",
            "epoch 187 | validation loss: 5.817229560177241e-05\n",
            "epoch 188 | validation loss: 8.398694782434697e-05\n",
            "epoch 189 | validation loss: 2.6844077605606538e-05\n",
            "epoch 190 | validation loss: 4.5707929822518576e-05\n",
            "epoch 191 | validation loss: 5.780871976769829e-05\n",
            "epoch 192 | validation loss: 3.349190494970701e-05\n",
            "epoch 193 | validation loss: 4.7348833114873355e-05\n",
            "epoch 194 | validation loss: 0.00010315385978693874\n",
            "epoch 195 | validation loss: 0.00013886053654589756\n",
            "epoch 196 | validation loss: 3.164205743035252e-05\n",
            "epoch 197 | validation loss: 3.840850545852196e-05\n",
            "epoch 198 | validation loss: 2.129105087078642e-05 | new best model!\n",
            "epoch 199 | validation loss: 2.476398386923912e-05\n",
            "epoch 200 | validation loss: 2.5816793010692762e-05\n",
            "epoch 201 | validation loss: 2.1234529559218833e-05 | new best model!\n",
            "epoch 202 | validation loss: 2.5675343636066827e-05\n",
            "epoch 203 | validation loss: 5.693098981802385e-05\n",
            "epoch 204 | validation loss: 5.116470882577622e-05\n",
            "epoch 205 | validation loss: 3.7150181307359685e-05\n",
            "epoch 206 | validation loss: 0.00011553145824499162\n",
            "epoch 207 | validation loss: 5.3816716380034837e-05\n",
            "epoch 208 | validation loss: 2.1351416242915938e-05\n",
            "epoch 209 | validation loss: 3.725217452808595e-05\n",
            "epoch 210 | validation loss: 2.3184151522010715e-05\n",
            "epoch 211 | validation loss: 3.819600436015337e-05\n",
            "epoch 212 | validation loss: 2.1938408061877063e-05\n",
            "epoch 213 | validation loss: 2.7052642540833665e-05\n",
            "epoch 214 | validation loss: 3.446855518112508e-05\n",
            "epoch 215 | validation loss: 2.3141953355272043e-05\n",
            "epoch 216 | validation loss: 3.342160984435135e-05\n",
            "epoch 217 | validation loss: 4.59766093367112e-05\n",
            "epoch 218 | validation loss: 1.9190047039176405e-05 | new best model!\n",
            "epoch 219 | validation loss: 3.011570116281551e-05\n",
            "epoch 220 | validation loss: 3.067376707934143e-05\n",
            "epoch 221 | validation loss: 3.3870526619596474e-05\n",
            "epoch 222 | validation loss: 5.581356643753159e-05\n",
            "epoch 223 | validation loss: 4.9815886411254595e-05\n",
            "epoch 224 | validation loss: 3.234354348283627e-05\n",
            "epoch 225 | validation loss: 2.079088436143398e-05\n",
            "epoch 226 | validation loss: 2.425278038182504e-05\n",
            "epoch 227 | validation loss: 7.213042105799364e-05\n",
            "epoch 228 | validation loss: 1.8889833167382415e-05 | new best model!\n",
            "epoch 229 | validation loss: 2.6249042450997096e-05\n",
            "epoch 230 | validation loss: 8.228090272106263e-05\n",
            "epoch 231 | validation loss: 2.5705645774516078e-05\n",
            "epoch 232 | validation loss: 5.307422751294715e-05\n",
            "epoch 233 | validation loss: 1.897141723767553e-05\n",
            "epoch 234 | validation loss: 1.8291476010290353e-05 | new best model!\n",
            "epoch 235 | validation loss: 2.1908340548699406e-05\n",
            "epoch 236 | validation loss: 2.256335215276652e-05\n",
            "epoch 237 | validation loss: 2.912101747045069e-05\n",
            "epoch 238 | validation loss: 2.479440773380962e-05\n",
            "epoch 239 | validation loss: 2.4366151787294905e-05\n",
            "epoch 240 | validation loss: 1.894095735873894e-05\n",
            "epoch 241 | validation loss: 2.3733918169455137e-05\n",
            "epoch 242 | validation loss: 0.00012232145023257805\n",
            "epoch 243 | validation loss: 3.504566237617317e-05\n",
            "epoch 244 | validation loss: 5.91528405493591e-05\n",
            "epoch 245 | validation loss: 4.7414039792264316e-05\n",
            "epoch 246 | validation loss: 2.3133741576332666e-05\n",
            "epoch 247 | validation loss: 6.254844421554091e-05\n",
            "epoch 248 | validation loss: 2.3843020861542235e-05\n",
            "epoch 249 | validation loss: 2.7816933149032203e-05\n",
            "epoch 250 | validation loss: 4.239374330999064e-05\n",
            "epoch 251 | validation loss: 1.9294963749416638e-05\n",
            "epoch 252 | validation loss: 2.6139844700472687e-05\n",
            "epoch 253 | validation loss: 2.2787476560941805e-05\n",
            "epoch 254 | validation loss: 3.1035094350175185e-05\n",
            "epoch 255 | validation loss: 2.2957869759920868e-05\n",
            "epoch 256 | validation loss: 3.8455255110290765e-05\n",
            "epoch 257 | validation loss: 3.062005725951167e-05\n",
            "epoch 258 | validation loss: 3.064862085011555e-05\n",
            "epoch 259 | validation loss: 2.1005700968349368e-05\n",
            "epoch 260 | validation loss: 5.570191745261711e-05\n",
            "epoch 261 | validation loss: 3.916497637744344e-05\n",
            "epoch 262 | validation loss: 5.771516966238518e-05\n",
            "epoch 263 | validation loss: 3.8817505047258495e-05\n",
            "epoch 264 | validation loss: 2.846941281729544e-05\n",
            "epoch 265 | validation loss: 2.8032304791330326e-05\n",
            "epoch 266 | validation loss: 0.00011112033605188896\n",
            "epoch 267 | validation loss: 2.4829161572727442e-05\n",
            "epoch 268 | validation loss: 8.753164092922816e-05\n",
            "epoch 269 | validation loss: 6.13568759685482e-05\n",
            "epoch 270 | validation loss: 2.048276472318581e-05\n",
            "epoch 271 | validation loss: 6.415459912594608e-05\n",
            "epoch 272 | validation loss: 2.2286409521906796e-05\n",
            "epoch 273 | validation loss: 3.711098945911674e-05\n",
            "epoch 274 | validation loss: 3.226263376226208e-05\n",
            "epoch 275 | validation loss: 2.4405357570295954e-05\n",
            "epoch 276 | validation loss: 6.0546383143186434e-05\n",
            "epoch 277 | validation loss: 9.156715584270257e-05\n",
            "epoch 278 | validation loss: 2.8072512155371703e-05\n",
            "epoch 279 | validation loss: 4.2666304157786984e-05\n",
            "epoch 280 | validation loss: 2.798696794213486e-05\n",
            "epoch 281 | validation loss: 2.8978789730769806e-05\n",
            "epoch 282 | validation loss: 1.9324232701884675e-05\n",
            "epoch 283 | validation loss: 8.587177840776608e-05\n",
            "epoch 284 | validation loss: 2.6142617181384204e-05\n",
            "epoch 285 | validation loss: 2.2462448230596366e-05\n",
            "early stopping!\n",
            "Average test error: 565.5082397460938\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rdr/Documents/Trading_bot/.venv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 | validation loss: 0.03747114871761629\n",
            "epoch 1 | validation loss: 0.07045771128364972\n",
            "epoch 2 | validation loss: 0.08580687801752772\n",
            "epoch 3 | validation loss: 0.05321381241083145\n",
            "epoch 4 | validation loss: 0.024651128532631055\n",
            "epoch 5 | validation loss: 0.013367949851921626\n",
            "epoch 6 | validation loss: 0.008483010926283896\n",
            "epoch 7 | validation loss: 0.006090152468199709\n",
            "epoch 8 | validation loss: 0.005334101376190249\n",
            "epoch 9 | validation loss: 0.0039941875137239026\n",
            "epoch 10 | validation loss: 0.00358812884535707\n",
            "epoch 11 | validation loss: 0.003482550328564165\n",
            "epoch 12 | validation loss: 0.0027151019686633454\n",
            "epoch 13 | validation loss: 0.0027636323294635595\n",
            "epoch 14 | validation loss: 0.002299312754725439\n",
            "epoch 15 | validation loss: 0.0018600386387593712\n",
            "epoch 16 | validation loss: 0.0020581679912408746\n",
            "epoch 17 | validation loss: 0.0016941077005217917\n",
            "epoch 18 | validation loss: 0.0016667777339794806\n",
            "epoch 19 | validation loss: 0.0014784201604405617\n",
            "epoch 20 | validation loss: 0.0013477396621185886\n",
            "epoch 21 | validation loss: 0.0013788398994490439\n",
            "epoch 22 | validation loss: 0.001623686440455328\n",
            "epoch 23 | validation loss: 0.0014493405230626064\n",
            "epoch 24 | validation loss: 0.0009979672125025121\n",
            "epoch 25 | validation loss: 0.001046687074579365\n",
            "epoch 26 | validation loss: 0.001263766998558172\n",
            "epoch 27 | validation loss: 0.0011417299527758068\n",
            "epoch 28 | validation loss: 0.0009106544210745986\n",
            "epoch 29 | validation loss: 0.0008462106598017272\n",
            "epoch 30 | validation loss: 0.0007823053160141821\n",
            "epoch 31 | validation loss: 0.0008512757659835708\n",
            "epoch 32 | validation loss: 0.0007093220111918137\n",
            "epoch 33 | validation loss: 0.0007676547143741377\n",
            "epoch 34 | validation loss: 0.000787223617537945\n",
            "epoch 35 | validation loss: 0.0008145893220249231\n",
            "epoch 36 | validation loss: 0.0008236167723225662\n",
            "epoch 37 | validation loss: 0.0007904018239059951\n",
            "epoch 38 | validation loss: 0.0006678184803864237\n",
            "epoch 39 | validation loss: 0.0006689931300830462\n",
            "epoch 40 | validation loss: 0.0007755404372541566\n",
            "epoch 41 | validation loss: 0.0007246854791966533\n",
            "epoch 42 | validation loss: 0.000660268592972508\n",
            "epoch 43 | validation loss: 0.0006302604827266935\n",
            "epoch 44 | validation loss: 0.0006770827569978012\n",
            "epoch 45 | validation loss: 0.0006280888566314908\n",
            "epoch 46 | validation loss: 0.0005827341240417029\n",
            "epoch 47 | validation loss: 0.0007726529376148912\n",
            "epoch 48 | validation loss: 0.0005051545111070611\n",
            "epoch 49 | validation loss: 0.00046665174646217826\n",
            "epoch 50 | validation loss: 0.000505848412753299\n",
            "early stopping!\n",
            "Average test error: 565.5082397460938\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rdr/Documents/Trading_bot/.venv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:917: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 | validation loss: 0.07240971603563853\n",
            "epoch 1 | validation loss: 0.08577950884188924\n",
            "epoch 2 | validation loss: 0.08394483849406242\n",
            "epoch 3 | validation loss: 0.06456244417599269\n",
            "epoch 4 | validation loss: 0.02674811559596232\n",
            "epoch 5 | validation loss: 0.011649663809553854\n",
            "epoch 6 | validation loss: 0.004411954285127909\n",
            "epoch 7 | validation loss: 0.0029602793199176502\n",
            "epoch 8 | validation loss: 0.0018518849371633092\n",
            "epoch 9 | validation loss: 0.0021329005477517577\n",
            "epoch 10 | validation loss: 0.0012304186852166563\n",
            "epoch 11 | validation loss: 0.0011569405963070625\n",
            "epoch 12 | validation loss: 0.0018618420498179539\n",
            "epoch 13 | validation loss: 0.000931308970653585\n",
            "epoch 14 | validation loss: 0.0009011529436975252\n",
            "epoch 15 | validation loss: 0.0008778388426955124\n",
            "epoch 16 | validation loss: 0.0009333158180067715\n",
            "epoch 17 | validation loss: 0.0009493267420371662\n",
            "epoch 18 | validation loss: 0.0010251514109508467\n",
            "epoch 19 | validation loss: 0.0011516671636075313\n",
            "epoch 20 | validation loss: 0.0005889186202564035\n",
            "epoch 21 | validation loss: 0.0008158611513603578\n",
            "epoch 22 | validation loss: 0.0007990933772816788\n",
            "epoch 23 | validation loss: 0.00042116918016940223\n",
            "epoch 24 | validation loss: 0.00042143753645567425\n",
            "epoch 25 | validation loss: 0.0006107389973684414\n",
            "epoch 26 | validation loss: 0.00038570506200942745\n",
            "epoch 27 | validation loss: 0.0009132023478741758\n",
            "epoch 28 | validation loss: 0.0005878824447011409\n",
            "epoch 29 | validation loss: 0.0005154514211816215\n",
            "epoch 30 | validation loss: 0.0002959373871687733\n",
            "epoch 31 | validation loss: 0.0003483264498364796\n",
            "epoch 32 | validation loss: 0.0003900254834141898\n",
            "epoch 33 | validation loss: 0.00036107069744113166\n",
            "epoch 34 | validation loss: 0.0002839022798590512\n",
            "epoch 35 | validation loss: 0.0002267560058888713\n",
            "epoch 36 | validation loss: 0.0002748282526486686\n",
            "epoch 37 | validation loss: 0.0005333556096697326\n",
            "epoch 38 | validation loss: 0.00039414681876743477\n",
            "epoch 39 | validation loss: 0.00022992266089464204\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Parameter update\u001b[39;00m\n\u001b[1;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/Documents/Trading_bot/.venv/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/Trading_bot/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/Trading_bot/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for fold, (train_indices, val_indices) in enumerate(kf.split(X_train)):\n",
        "    print(f'----------\\nFOLD {fold}')\n",
        "\n",
        "    train_loader, val_loader = data_process.create_fold_sets(train_indices, val_indices)\n",
        "\n",
        "    results[fold].input_scaler  = data_process.input_scaler\n",
        "    results[fold].output_scaler = data_process.output_scaler\n",
        "\n",
        "    hp_results = []\n",
        "    for learning_rate, hidden_size, num_fc, num_lstm in HP_combinations:\n",
        "        hp_results.append(Hyperparams(learning_rate, hidden_size, num_fc, num_lstm))\n",
        "\n",
        "        # Create LSTM object and move it to the GPU\n",
        "        lstm = LSTM(output_size, input_size, hidden_size, num_lstm, num_fc, device).to(device)\n",
        "\n",
        "        # Initialize optimizer\n",
        "        optimizer = torch.optim.Adam(lstm.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "        epochs_wo_improvement = 0\n",
        "        for epoch in range(NUM_EPOCHS):\n",
        "            if epochs_wo_improvement > 50:\n",
        "                print('early stopping!')\n",
        "                break\n",
        "\n",
        "            for i, data in enumerate(train_loader, 0):\n",
        "                inputs, targets = data\n",
        "                targets = targets.reshape(targets.shape[0],1)\n",
        "\n",
        "                inputs  = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "\n",
        "                # Zero the gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = lstm.forward(inputs)\n",
        "                # Compute loss\n",
        "                loss = criterion(outputs, targets)\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                # Parameter update\n",
        "                optimizer.step()\n",
        "\n",
        "            val_results = ModelInfo()\n",
        "            val_results.loss = 0\n",
        "            val_results.params = deepcopy(lstm.state_dict())\n",
        "            val_results.model  = deepcopy(lstm)\n",
        "            with torch.no_grad():\n",
        "                for i, val_data in enumerate(val_loader, 0):\n",
        "                    val_inputs, val_targets = val_data\n",
        "                    val_targets = val_targets.reshape(val_targets.shape[0], 1)\n",
        "\n",
        "                    val_inputs  = val_inputs.to(device)\n",
        "                    val_targets = val_targets.to(device)\n",
        "\n",
        "                    val_outputs = lstm.forward(val_inputs)\n",
        "                    val_loss = criterion(val_outputs, val_targets)\n",
        "\n",
        "                    for i in range(val_inputs.shape[0]):\n",
        "                        single_input = val_inputs[i].flatten()\n",
        "                        single_label = val_targets[i]\n",
        "                        single_output = val_outputs[i]\n",
        "\n",
        "                        val_results.inputs.append(single_input)\n",
        "                        val_results.labels.append(single_label)\n",
        "                        val_results.outputs.append(single_output)\n",
        "\n",
        "                    val_results.loss += float(val_loss.item())\n",
        "            val_results.loss /= len(val_loader)\n",
        "\n",
        "            if results[fold].loss > val_results.loss:\n",
        "                val_results.train_loader  = deepcopy(train_loader)\n",
        "                val_results.val_loader    = deepcopy(val_loader)\n",
        "                val_results.input_scaler  = deepcopy(results[fold].input_scaler)\n",
        "                val_results.output_scaler = deepcopy(results[fold].output_scaler)\n",
        "                results[fold]             = deepcopy(val_results)\n",
        "\n",
        "                print(f'epoch {epoch} | validation loss: {val_results.loss} | new best model!')\n",
        "                epochs_wo_improvement = 0\n",
        "            else:\n",
        "                print(f'epoch {epoch} | validation loss: {val_results.loss}')\n",
        "                epochs_wo_improvement += 1\n",
        "\n",
        "        X_test_norm, y_test_norm, _, _ = data_process.normalize(X_test, y_test, results[fold].input_scaler, results[fold].output_scaler, fit=0)\n",
        "\n",
        "        lstm_test = results[fold].model\n",
        "\n",
        "        lstm_test.load_state_dict(results[fold].params)\n",
        "\n",
        "        # Set the model to evaluation mode\n",
        "        lstm_test.eval()\n",
        "\n",
        "        X_test_norm = X_test_norm.to(device)\n",
        "        test_output = lstm_test.forward(X_test_norm).to(device)\n",
        "\n",
        "        test_output_np = test_output.cpu().data.numpy()\n",
        "        test_labels_np = y_test_norm.data.numpy()\n",
        "        test_labels_np = test_labels_np.reshape(test_labels_np.shape[0], 1)\n",
        "\n",
        "        X_plot = results[fold].input_scaler.inverse_transform(test_output_np)\n",
        "        y_plot = results[fold].output_scaler.inverse_transform(test_labels_np)\n",
        "\n",
        "        errors = np.abs(X_plot - y_plot)\n",
        "        avg_error = np.mean(errors)\n",
        "        print(f'Average test error: {avg_error}\\n')\n",
        "\n",
        "        hp_results[-1].avg_val_error\n",
        "\n",
        "    best_avg_val_loss = np.inf\n",
        "    best_config       = None\n",
        "    for hp_result in hp_results:\n",
        "        if hp_result.avg_val_loss < best_avg_val_loss:\n",
        "            best_config = hp_result\n",
        "            best_avg_val_loss = hp_result.avg_val_loss\n",
        "    print('----------')\n",
        "    print(f'Best config:')\n",
        "    print(f'    learning rate       : {hp_result.learning_rate}')\n",
        "    print(f'    hidden size         : {hp_result.hidden_size}')\n",
        "    print(f'    num of fc features  : {hp_result.hidden_size}')\n",
        "    print(f'    num of lstm layers  : {hp_result.hidden_size}')\n",
        "    print(f'    AVERAGE VAL ERROR   : {hp_result.avg_val_loss}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSTUZPquerOT"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hp_results = []\n",
        "for seq_length, batch_size, lr, fnn_feats, h_size, num_lstms in HP_combinations:\n",
        "    data_process = DataProcessing(seq_length, batch_size)\n",
        "    data_process.get_process_data()\n",
        "\n",
        "    train_loader, test_loader = data_process.create_fold_sets()\n",
        "\n",
        "    hp_results.append(Hyperparams(learning_rate, hidden_size, num_fc, num_lstm))\n",
        "\n",
        "    # Create LSTM object and move it to the GPU\n",
        "    lstm = LSTM(output_size, input_size, hidden_size, num_lstm, num_fc, device).to(device)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = torch.optim.Adam(lstm.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    epochs_wo_improvement = 0\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        if epochs_wo_improvement > 50:\n",
        "            print('early stopping!')\n",
        "            break\n",
        "\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, targets = data\n",
        "            targets = targets.reshape(targets.shape[0],1)\n",
        "\n",
        "            inputs  = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = lstm.forward(inputs)\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, targets)\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            # Parameter update\n",
        "            optimizer.step()\n",
        "\n",
        "        val_results = ModelInfo()\n",
        "        val_results.loss = 0\n",
        "        val_results.params = deepcopy(lstm.state_dict())\n",
        "        val_results.model  = deepcopy(lstm)\n",
        "        with torch.no_grad():\n",
        "            for i, val_data in enumerate(val_loader, 0):\n",
        "                val_inputs, val_targets = val_data\n",
        "                val_targets = val_targets.reshape(val_targets.shape[0], 1)\n",
        "\n",
        "                val_inputs  = val_inputs.to(device)\n",
        "                val_targets = val_targets.to(device)\n",
        "\n",
        "                val_outputs = lstm.forward(val_inputs)\n",
        "                val_loss = criterion(val_outputs, val_targets)\n",
        "\n",
        "                for i in range(val_inputs.shape[0]):\n",
        "                    single_input = val_inputs[i].flatten()\n",
        "                    single_label = val_targets[i]\n",
        "                    single_output = val_outputs[i]\n",
        "\n",
        "                    val_results.inputs.append(single_input)\n",
        "                    val_results.labels.append(single_label)\n",
        "                    val_results.outputs.append(single_output)\n",
        "\n",
        "                val_results.loss += float(val_loss.item())\n",
        "        val_results.loss /= len(val_loader)\n",
        "\n",
        "        if results[fold].loss > val_results.loss:\n",
        "            val_results.train_loader  = deepcopy(train_loader)\n",
        "            val_results.val_loader    = deepcopy(val_loader)\n",
        "            val_results.input_scaler  = deepcopy(results[fold].input_scaler)\n",
        "            val_results.output_scaler = deepcopy(results[fold].output_scaler)\n",
        "            results[fold]             = deepcopy(val_results)\n",
        "\n",
        "            print(f'epoch {epoch} | validation loss: {val_results.loss} | new best model!')\n",
        "            epochs_wo_improvement = 0\n",
        "        else:\n",
        "            print(f'epoch {epoch} | validation loss: {val_results.loss}')\n",
        "            epochs_wo_improvement += 1\n",
        "\n",
        "    X_test_norm, y_test_norm, _, _ = data_process.normalize(X_test, y_test, results[fold].input_scaler, results[fold].output_scaler, fit=0)\n",
        "\n",
        "    lstm_test = results[fold].model\n",
        "\n",
        "    lstm_test.load_state_dict(results[fold].params)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    lstm_test.eval()\n",
        "\n",
        "    X_test_norm = X_test_norm.to(device)\n",
        "    test_output = lstm_test.forward(X_test_norm).to(device)\n",
        "\n",
        "    test_output_np = test_output.cpu().data.numpy()\n",
        "    test_labels_np = y_test_norm.data.numpy()\n",
        "    test_labels_np = test_labels_np.reshape(test_labels_np.shape[0], 1)\n",
        "\n",
        "    X_plot = results[fold].input_scaler.inverse_transform(test_output_np)\n",
        "    y_plot = results[fold].output_scaler.inverse_transform(test_labels_np)\n",
        "\n",
        "    errors = np.abs(X_plot - y_plot)\n",
        "    avg_error = np.mean(errors)\n",
        "    print(f'Average test error: {avg_error}\\n')\n",
        "\n",
        "    hp_results[-1].avg_val_error\n",
        "\n",
        "best_avg_val_loss = np.inf\n",
        "best_config       = None\n",
        "for hp_result in hp_results:\n",
        "    if hp_result.avg_val_loss < best_avg_val_loss:\n",
        "        best_config = hp_result\n",
        "        best_avg_val_loss = hp_result.avg_val_loss\n",
        "print('----------')\n",
        "print(f'Best config:')\n",
        "print(f'    learning rate       : {hp_result.learning_rate}')\n",
        "print(f'    hidden size         : {hp_result.hidden_size}')\n",
        "print(f'    num of fc features  : {hp_result.hidden_size}')\n",
        "print(f'    num of lstm layers  : {hp_result.hidden_size}')\n",
        "print(f'    AVERAGE VAL ERROR   : {hp_result.avg_val_loss}')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
