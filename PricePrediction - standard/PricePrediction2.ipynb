{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.amp import GradScaler\n",
    "from torch.utils.data import Subset, DataLoader, TensorDataset\n",
    "from torch.amp import autocast\n",
    "import copy\n",
    "\n",
    "from tests import Tester\n",
    "tester = Tester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "NUM_EPOCHS      = 1000\n",
    "LEARNING_RATE   = 0.001         \n",
    "\n",
    "HIDDEN_SIZE     = 64            # Dimensionality of hidden units\n",
    "NUM_FC_FEATURES = 64           # Number of output features, first FF layer\n",
    "NUM_LSTM_LAYERS = 5             # Number of LSTM Layers\n",
    "\n",
    "SEQ_LENGTH      = 60             # Length of inputs sequences\n",
    "BATCH_SIZE      = 15\n",
    "\n",
    "# Regular parameters\n",
    "input_size  = 1      # Number of input features\n",
    "output_size = 1      # Number of output features\n",
    "\n",
    "criterion = nn.MSELoss()    # Set MSE as loss function\n",
    "\n",
    "nr_folds = 10               # Number of folds in K-fold cross validation\n",
    "kf = KFold(n_splits = nr_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "df = yf.download(\"BTC-USD\", start=\"2017-1-1\", end=\"2024-1-1\", interval=\"1d\")\n",
    "\n",
    "# Get list of closing prices\n",
    "closing_prices = list(df.iloc[:, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate inputs (X) and labels (y)\n",
    "\n",
    "def create_dataset(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data)-seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Get list of closing prices\n",
    "closing_prices = list(df.iloc[:, 3])\n",
    "\n",
    "X, y = create_dataset(closing_prices, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.test_labels(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seperate training and test set\n",
    "\n",
    "test_indices  = random.sample(range(len(X)), 300)\n",
    "X_test = torch.tensor(X[test_indices], dtype=torch.float32)\n",
    "y_test = torch.tensor(y[test_indices], dtype=torch.float32)\n",
    "\n",
    "train_indices = [i for i in range(len(X)) if i not in test_indices]\n",
    "X_train = torch.tensor(X[train_indices], dtype=torch.float32)\n",
    "y_train = torch.tensor(y[train_indices], dtype=torch.float32)\n",
    "\n",
    "# Reshape X,y into (nr_of_samples, seq_length, nr_of_features) -> necessary for LSTM\n",
    "X_test = torch.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "y_test = torch.reshape(y_test, (y_test.shape[0], 1, 1))\n",
    "\n",
    "X_train = torch.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "y_train = torch.reshape(y_train, (y_train.shape[0], 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scalers\n",
    "feature_scaler = MinMaxScaler()\n",
    "label_scaler = MinMaxScaler()\n",
    "\n",
    "def normalize(X, y, fit=0):\n",
    "    # Reshape training data to 2D for normalization\n",
    "    X_reshaped = X.reshape(-1, X.shape[-1])  # shape: (nr_train_sequences * sequence_length, nr_of_features)\n",
    "    y_reshaped = y.reshape(-1, y.shape[-1])  # shape: (nr_train_sequences * sequence_length, nr_of_features)\n",
    "\n",
    "    if fit == 1:\n",
    "        # Fit scaler to training data and transform training data\n",
    "        X_normalized = feature_scaler.fit_transform(X_reshaped)\n",
    "        y_normalized = label_scaler.fit_transform(y_reshaped)\n",
    "    else:\n",
    "        # Transform test data\n",
    "        X_normalized = feature_scaler.transform(X_reshaped)\n",
    "        y_normalized = label_scaler.transform(y_reshaped)\n",
    "\n",
    "    if np.min(X_normalized) >= 0: print(f'Value smaller than 0: {np.min(X_normalized)}')\n",
    "    if np.max(X_normalized) >= 0: print(f'Value greater than 1: {np.min(X_normalized)}')\n",
    "\n",
    "    # Reshape back to 3D\n",
    "    X_normalized = X_normalized.reshape(X.shape)\n",
    "    y_normalized = y_normalized.reshape(y.shape)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X_normalized, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_normalized, dtype=torch.float32)\n",
    "\n",
    "    return X_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rdr/Documents/Trading_bot/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/rdr/Documents/Trading_bot/.venv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set device to CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize gradscaler (AMP) -> gpu accelerations\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, output_size, input_size, hidden_size, num_layers, nr_fc_features):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size    = hidden_size\n",
    "        self.num_layers     = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size, nr_fc_features)\n",
    "        self.fc2 = nn.Linear(nr_fc_features, output_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        h0 = torch.zeros(self.num_layers, X.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, X.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(X)\n",
    "\n",
    "        # Only use the final hidden state of the final layer as input to the FF layers\n",
    "        hn = hn[-1]\n",
    "\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Struct to store model info\n",
    "class ModelInfo():\n",
    "    def __init__(self):\n",
    "        self.inputs     = []\n",
    "        self.labels     = []\n",
    "        self.outputs    = []\n",
    "        self.loss       = float(0)\n",
    "        self.model      = None\n",
    "        self.train_loader = None\n",
    "        self.val_loader   = None\n",
    "\n",
    "# Store best model of every fold\n",
    "results = {i:ModelInfo() for i in range(nr_folds)}\n",
    "for i in range(nr_folds): results[i].loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "FOLD 0\n",
      "Value smaller than 0: 0.0\n",
      "Value greater than 1: 0.0\n",
      "Value greater than 1: -0.03690612662642104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rdr/Documents/Trading_bot/.venv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | validation loss: 0.02841144111007452 | new best model!\n",
      "epoch 1 | validation loss: 0.029578591883182525\n",
      "epoch 2 | validation loss: 0.08565227116147676\n",
      "epoch 3 | validation loss: 0.0648449033498764\n",
      "epoch 4 | validation loss: 0.09808458338181178\n",
      "epoch 5 | validation loss: 0.08845160553852717\n",
      "epoch 6 | validation loss: 0.11128657956918081\n",
      "epoch 7 | validation loss: 0.07031153763333957\n",
      "epoch 8 | validation loss: 0.07629146873950958\n",
      "epoch 9 | validation loss: 0.0904318630695343\n",
      "epoch 10 | validation loss: 0.09358748197555541\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Unscales gradients and calls or skips optimizer.step()\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Unscales gradients and calls or skips optimizer.step()\u001b[39;00m\n\u001b[1;32m     54\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n",
      "File \u001b[0;32m~/Documents/Trading_bot/.venv/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Trading_bot/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Trading_bot/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold, (train_indices, val_indices) in enumerate(kf.split(X_train)):\n",
    "    print(f'----------\\nFOLD {fold}')\n",
    "\n",
    "    # Normalize Training and Validation sets\n",
    "    X_train_fold, y_train_fold  = X_train[train_indices], y_train[train_indices]\n",
    "    X_val_fold, y_val_fold      = X_train[val_indices], y_train[val_indices]\n",
    "\n",
    "    X_train_fold, y_train_fold = normalize(X_train_fold, y_train_fold, fit=1)\n",
    "    X_val_fold, y_val_fold     = normalize(X_val_fold, y_val_fold, fit=0)\n",
    "\n",
    "\n",
    "    # Combine inputs and labels\n",
    "    train_dataset_fold  = TensorDataset(X_train_fold, y_train_fold)\n",
    "    val_dataset_fold    = TensorDataset(X_val_fold, y_val_fold)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset_fold, batch_size=BATCH_SIZE, \n",
    "                              shuffle=True, num_workers=1, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_dataset_fold, batch_size=BATCH_SIZE, \n",
    "                              shuffle=False, num_workers=1, pin_memory=True)\n",
    "\n",
    "    # Create LSTM object and move it to the GPU\n",
    "    lstm = LSTM(output_size, input_size, HIDDEN_SIZE, NUM_LSTM_LAYERS, NUM_FC_FEATURES).to(device)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(lstm.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    epochs_wo_improvement = 0\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        if epochs_wo_improvement > 50:\n",
    "            print('early stopping!')\n",
    "            break\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, targets = data\n",
    "            targets = targets.reshape(targets.shape[0],1)\n",
    "\n",
    "            inputs  = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                # Forward pass\n",
    "                outputs = lstm.forward(inputs)\n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            # Unscales gradients and calls or skips optimizer.step()\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Unscales gradients and calls or skips optimizer.step()\n",
    "            scaler.step(optimizer)\n",
    "            \n",
    "            # Updates the scale for next iteration\n",
    "            scaler.update()\n",
    "\n",
    "        val_results = ModelInfo()\n",
    "        val_results.loss = 0 \n",
    "        val_results.model = copy.copy(lstm.state_dict())\n",
    "        with torch.no_grad():\n",
    "            for i, val_data in enumerate(val_loader, 0):\n",
    "                val_inputs, val_targets = val_data\n",
    "                val_targets = val_targets.reshape(val_targets.shape[0], 1)\n",
    "\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_targets = val_targets.to(device)\n",
    "\n",
    "                val_outputs = lstm.forward(val_inputs)\n",
    "                val_loss = criterion(val_outputs, val_targets)\n",
    "\n",
    "                for i in range(val_inputs.shape[0]):\n",
    "                    single_input = val_inputs[i].flatten()\n",
    "                    single_label = val_targets[i]\n",
    "                    single_output = val_outputs[i]\n",
    "\n",
    "                    val_results.inputs.append(single_input)\n",
    "                    val_results.labels.append(single_label)\n",
    "                    val_results.outputs.append(single_output)\n",
    "\n",
    "                val_results.loss += float(val_loss.item())\n",
    "        val_results.loss /= len(val_loader)\n",
    "\n",
    "        if results[fold].loss > val_results.loss:\n",
    "            val_results.train_loader = copy.copy(train_loader)\n",
    "            val_results.val_loader = copy.copy(val_loader)\n",
    "            results[fold] = copy.copy(val_results)\n",
    "            print(f'epoch {epoch} | validation loss: {val_results.loss} | new best model!')\n",
    "            epochs_wo_improvement = 0\n",
    "        else:\n",
    "            print(f'epoch {epoch} | validation loss: {val_results.loss}')\n",
    "            epochs_wo_improvement += 1\n",
    "\n",
    "\n",
    "\n",
    "    X_test_norm, y_test_norm = normalize(X_test, y_test)\n",
    "    \n",
    "    lstm_test = LSTM(output_size, input_size, HIDDEN_SIZE, NUM_LSTM_LAYERS, NUM_FC_FEATURES).to(device)\n",
    "\n",
    "    lstm_test.load_state_dict(results[fold].model)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    lstm_test.eval()\n",
    "\n",
    "    X_test_norm = X_test_norm.to(device)\n",
    "    test_output = lstm_test.forward(X_test_norm).to(device)\n",
    "\n",
    "    test_output_np = test_output.cpu().data.numpy()\n",
    "    test_labels_np = y_test_norm.data.numpy()\n",
    "    test_labels_np = test_labels_np.reshape(test_labels_np.shape[0], 1)\n",
    "\n",
    "    X_plot = feature_scaler.inverse_transform(test_output_np)\n",
    "    y_plot = label_scaler.inverse_transform(test_labels_np)\n",
    "\n",
    "    errors = np.abs(X_plot - y_plot)\n",
    "    avg_error = np.mean(errors)\n",
    "    print(f'Average test error: {avg_error}\\n')\n",
    "\n",
    "    plt.scatter(np.arange(len(errors)), errors)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
